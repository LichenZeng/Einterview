第三题：有N个样本，每个样本都是D维的，使用KNN分类算法，采用欧式距离，（1）KNN分类的的思想是，计算待分类样本与训练集中所有样本的距离，找出距离最小的k个，计算那个标签出现的次数最多，将出现最多的标签作为样本的标签。计算距离的复杂度是O(M*D)，排序最快得是O（M*log(M）），找出现最多的标签的复杂度是K。因此总的时间复杂度是O(N*D+N*log(N)+K))。
（2）KNN比较费时的步骤在于要计算与训练集中每一个样本的距离，为了减少计算量，可以每次从训练集中随机地选取一部分样本进行，作为分类的依据。假设每次取T个点，则时间复杂度变为O((T*D+T*log(T)+K))。当N很大时，选取的T可以远远小于N。

（3）记样本的真实标签为Y，分类结果记为X，X是随机变量，可以取训练集中所有出现的类标签。
如果K=N，则任意Y，X|Y=训练集中包含样本数最多的类的标签。
如果K=1，则分类结果是离待分类样本最近的训练集中的样本的类的标签。
现在，仍然没有梳理出这里的方差和偏差指的是什么？


用天平称量8个球的质量，其中有一个球比其它7个球质量大，就能将重球找出来。
具体操作过程如下：

1.标号：将8个球分别标记上1、2、3、4、5、6、7、8
2.分组：将标记好的8个球分别三组，A 组为 1、2、3三个球，B为4、5、6三个球，  C为7、8两个球。
3.操作：
（1）第一次称量： 天平左右两边分别放A组和B组球，会出现三种情况：

（I）第一种情况：A＝B，说明6个球的质量相同，重球在C组中。
（II）第二种情况：A＞B，说明A组中的3个球有重球，
进行第二次称量，将1球和2号球分别放入天平的两边，又会出现三种情况：
（A）1号球和2号球质量相等，说明3号球是重球；
（B）1号球的质量大于2号球质量，说明1号球是重球；
（C）2号球的质量大于1号球质量，说明2号球是重球。
（III）第三种情况：A＜B，说明B组中的球有重球，
进行第二次称量，将4号球和5号球分别放入天平的两边，又会出现三种情况：
（A）4号球和5号球质量相等，说明6号球是重球；
（B）4号球的质量大于5号球质量，说明4号球是重球；
（C）5号球的质量大于4号球质量，说明5号球是重球。
（2）在第一次称量，出现第一种情况，即A＝B，重球在C组时，
将7号球和8号球，分别放入天平的两边称量，会出现两种情况：
（I）7号球的质量大于8号球，说明7号球是重球；
（II）8号球的质量大于1号球，说明8号球是重球。
总之，至少称量两次，最多称量三次就能将重球找出来。


#include <stdio.h>
#define MAX 500
int matrix[MAX][MAX];
int min(int a, int b){
return a < b ? a : b;
}
int main()
{
int max = 1;
int m, n;
scanf("%d%d", &m,&n);
for(int i = 0; i < m; i++)
for(int j = 0; j < n; j++)
scanf("%d", &matrix[i][j]);
for(int i = 1; i < m; i++)
for(int j = 1; j < n; j++)
if(matrix[i][j] == 1)
{
int mmin = min(matrix[i - 1][j], matrix[i][j - 1]);
mmin = min(matrix[i - 1][j - 1], mmin);
matrix[i][j] = mmin + 1;
if(max < matrix[i][j])
max = matrix[i][j];
}
printf("%d", max);
return 0;
}


一个M*N的矩阵，元素取值1或0，问如何找到最大的正方形，其所有的元素都为1


为什么引入非线性激励函数？
解析：
第一，对于神经网络来说，网络的每一层相当于f(wx+b)=f(w'x)，对于线性函数，其实相当于f(x)=x，那么在线性激活函数下，每一层相当于用一个矩阵去乘以x，那么多层就是反复的用矩阵去乘以输入。根据矩阵的乘法法则，多个矩阵相乘得到一个大矩阵。所以线性激励函数下，多层网络与一层网络相当。比如，两层的网络f(W1*f(W2x))=W1W2x=Wx。
 
第二，非线性变换是深度学习有效的原因之一。原因在于非线性相当于对空间进行变换，变换完成后相当于对问题空间进行简化，原来线性不可解的问题现在变得可以解了。
 
下图可以很形象的解释这个问题，左图用一根线是无法划分的。经过一系列变换后，就变成线性可解的问题了。
 
如果不用激励函数（其实相当于激励函数是f(x) = x），在这种情况下你每一层输出都是上层输入的线性函数，很容易验证，无论你神经网络有多少层，输出都是输入的线性组合，与没有隐藏层效果相当，这种情况就是最原始的感知机（Perceptron）了。
 
正因为上面的原因，我们决定引入非线性函数作为激励函数，这样深层神经网络就有意义了（不再是输入的线性组合，可以逼近任意函数）。最早的想法是sigmoid函数或者tanh函数，输出有界，很容易充当下一层输入（以及一些人的生物解释）。


#include <stdlib.h>

int strcmp(char *source, char *dest)
{
    while(*source == *dest && *source != '\0' && *dest != '\0')
    {
       source++;
       dest++;
    }
    if (*source =='\0' && *dest == '\0')
      return 0;
    else
      return -1;
}

int main()
{
    char * str1 = "abcde";
    char * str2 = "abcde";
    printf("ret = %d", mystrcmp(str1, str2));

    return 0;
}


然后就是人力资源面，接着立马背调，幸亏我提前给3个前同事说过了
