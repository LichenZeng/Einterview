Author: lichen_zeng@sina.cn
Date: 20181029
Subject: 阅读YOLO相关的博客


1. 具体的Specific
  阅读YOLO v1/v2/v3相关的博客，以便快速了解其算法的原理、调试步骤、训练步骤、使用等

2. 可衡量的Measurable
  YOLO原著文档或者其翻译版本；
  阅读16篇YOLO相关的博客，并记录每篇博客的页数和阅读时间；
  引申阅读检查算法相关的文章；
------
KERAS-YOLOV3的代码走读  -- 14P / 59Min  -- 代码分析较多，看得有点慢
YOLOv3：An Incremental Improvement全文翻译  -- 13P / 26Min
PASCAL VOC数据集分析  -- 5P / 11Min
KERAS-YOLOV3的数据增强  -- 3P / 9Min
目标检测 YOLO v3 训练 人脸检测模型 -- 11P / 21Min
YOLOv2 论文笔记  -- 19P / 39Min  -- 没有看进去，有点难，有干扰
基于KITTI数据集的KERAS-YOLOV3实践  -- 7P / 9Min
论文阅读：You Only Look Once: Unified, Real-Time Object Detection  -- 15P / 
【AI实战】动手训练自己的目标检测模型（YOLO篇） -- 8P / 12Min
【AI实战】手把手教你训练自己的目标检测模型（SSD篇）  -- 18P / 25Min
大话目标检测经典模型（RCNN、Fast RCNN、Faster RCNN）  -- 16P / 40Min  -- 不错
大话目标检测经典模型：Mark R-CNN  -- 10P / 16Min
Keras/Tensorflow+python+yolo3训练自己的数据集  -- 18P / 25Min  -- 公式还有解释，不错
Yolo系列其一：Yolo_v1  -- 11P / 25Min
YOLO v2（Darknet）训练自定义数据集使用记录  -- 11P / 9Min
YOLOv3: 训练自己的数据  -- 27P / 35Min
目标检测（九）--YOLO v1,v2,v3  -- 26P / 
【目标检测】yolov3为什么比yolov2好这么多
YOLO v3网络结构分析(https://blog.csdn.net/qq_37541097/article/details/81214953)

3. 可达到的Attainable
  目标细分后，清晰明了，可以达到

4. 目标相关的Relevant
  是熟悉YOLO算法的不二选择

5. 时间限制的Time-based
  阅读16篇YOLO相关的博客，耗时16 x 20Min = 320Min


# YOLO问题总结


## YOLO的v1和v2都不如SSD算法，原谅这么直白，原因是v1版本的448和v2版本的416都不如SSD的300，当然以上结论都是实验测的，v3版本的416应该比SSD512好，可见其性能。

## 对官方yolo做了实验，实验中，采用同一个视频、同一张显卡，在阈值为0.3的前提下，对比了v3和v2的测试效果之后，有了下面两个疑问：

  1. 为什么v3和v2版本的测试性能提高很大，但速度却没有降低？
  2. 为什么v3性能上能有这么大的改进？或者说为什么v3在没有提高输入数据分辨率的前提下，对小目标检测变得这么好？
  要回答上述两个问题，必须要看看作者发布的v3论文了，将v3和v2不一样的地方总结一下：

    loss不同：
	作者v3替换了v2的softmax loss 变成logistic loss，而且每个ground truth只匹配一个先验框。

    anchor bbox prior不同：
	v2作者用了5个anchor，一个折衷的选择，所以v3用了9个anchor，提高了IOU。

    detection的策略不同：
	v2只有一个detection，v3一下变成了3个，分别是一个下采样的，feature map为13*13，还有2个上采样的eltwise sum，feature map为26*26，52*52，也就是说v3的416版本已经用到了52的feature map，而v2把多尺度考虑到训练的data采样上，最后也只是用到了13的feature map，这应该是对小目标影响最大的地方。

    backbone不同：
	这和上一点是有关系的，v2的darknet-19变成了v3的darknet-53，为啥呢？就是需要上采样啊，卷积层的数量自然就多了，另外作者还是用了一连串的3*3、1*1卷积，3*3的卷积增加channel，而1*1的卷积在于压缩3*3卷积后的特征表示，这波操作很具有实用性，一增一减，效果棒棒。


## 为什么YOLOv2和YOLOv3的anchor大小有明显区别？

  在YOLOv2中，作者用最后一层feature map的相对大小来定义anchor大小。也就是说，在YOLOv2中，最后一层feature map大小为13X13，相对的anchor大小范围就在（0x0，13x13]，如果一个anchor大小是9x9，那么其在原图上的实际大小是288x288.

  而在YOLOv3中，作者又改用相对于原图的大小来定义anchor，anchor的大小为（0x0，input_w x input_h]。


## YOLO v3作者的Paper输入的是 256x256，所以最后一层的Feature map大小为 8x8，如果输入 416x416，则最后一层Feature map大小为 13x13，即32倍

  1. 卷积的strides默认为（1，1），padding默认为same，当strides为（2，2）时padding为valid
  2. 看完上图应该就能自己搭建出Darknet-53的网络结构了，上图是以输入图像256 x 256进行预训练来进行介绍的，常用的尺寸是416 x 416，都是32的倍数。下面我们再来分析下YOLOv3的特征提取器，看看究竟是在哪几层Features上做的预测。
